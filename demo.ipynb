{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown https://drive.google.com/open?id=1OPORTWB2cwd5YTVBX-NE8fsauZJWsrtW\n",
    "!mv ./duc_se.pth ./joints_detectors/Alphapose/models/sppe\n",
    "!gdown https://drive.google.com/open?id=1D47msNOOiJKvPOXlnpyzdKA3k6E97NTC\n",
    "!mv ./yolov3-spp.weights ./joints_detectors/Alphapose/models/yolo\n",
    "!wget https://dl.fbaipublicfiles.com/video-pose-3d/pretrained_h36m_detectron_coco.bin\n",
    "!mv ./pretrained_h36m_detectron_coco.bin ./checkpoint folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T09:27:44.505660Z",
     "start_time": "2021-03-03T09:27:38.439372Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9e5ca478fe20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcamera\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnchunkedGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python/VideoTo3dPoseAndBvh/common/camera.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquaternion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mqrot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqinverse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python/VideoTo3dPoseAndBvh/common/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from common.arguments import parse_args\n",
    "from common.camera import *\n",
    "from common.generators import UnchunkedGenerator\n",
    "from common.loss import *\n",
    "from common.model import *\n",
    "from common.utils import Timer, evaluate, add_path\n",
    "import cv2\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "from bvh_skeleton import openpose_skeleton,h36m_skeleton,cmu_skeleton,smartbody_skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joints_detectors.openpose.main import generate_kpts as open_pose\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "metadata = {'layout_name': 'coco', 'num_joints': 17, 'keypoints_symmetry': [[1, 3, 5, 7, 9, 11, 13, 15], [2, 4, 6, 8, 10, 12, 14, 16]]}\n",
    "\n",
    "add_path()\n",
    "\n",
    "\n",
    "# record time\n",
    "def ckpt_time(ckpt=None):\n",
    "    if not ckpt:\n",
    "        return time.time()\n",
    "    else:\n",
    "        return time.time() - float(ckpt), time.time()\n",
    "\n",
    "\n",
    "time0 = ckpt_time()\n",
    "\n",
    "\n",
    "def get_detector_2d(detector_name):\n",
    "    def get_alpha_pose():\n",
    "        from joints_detectors.Alphapose.gene_npz import generate_kpts as alpha_pose\n",
    "        return alpha_pose\n",
    "\n",
    "    def get_hr_pose():\n",
    "        from joints_detectors.hrnet.pose_estimation.video import generate_kpts as hr_pose\n",
    "        return hr_pose\n",
    "\n",
    "    detector_map = {\n",
    "        'alpha_pose': get_alpha_pose,\n",
    "        'hr_pose': get_hr_pose,\n",
    "        # 'open_pose': open_pose\n",
    "    }\n",
    "\n",
    "    assert detector_name in detector_map, f'2D detector: {detector_name} not implemented yet!'\n",
    "\n",
    "    return detector_map[detector_name]()\n",
    "\n",
    "\n",
    "class Skeleton:\n",
    "    def parents(self):\n",
    "        return np.array([-1, 0, 1, 2, 0, 4, 5, 0, 7, 8, 9, 8, 11, 12, 8, 14, 15])\n",
    "\n",
    "    def joints_right(self):\n",
    "        return [1, 2, 3, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    # 第一步：检测2D关键点\n",
    "    detector_2d = get_detector_2d(args.detector_2d)\n",
    "\n",
    "    assert detector_2d, 'detector_2d should be in ({alpha, hr, open}_pose)'\n",
    "\n",
    "    # 2D kpts loads or generate\n",
    "    if not args.input_npz:\n",
    "        video_name = args.viz_video\n",
    "        keypoints = detector_2d(video_name)\n",
    "    else:\n",
    "        npz = np.load(args.input_npz)\n",
    "        keypoints = npz['kpts']  # (N, 17, 2)\n",
    "\n",
    "    # 第二步：将2D关键点转换为3D关键点\n",
    "    keypoints_symmetry = metadata['keypoints_symmetry']\n",
    "    kps_left, kps_right = list(keypoints_symmetry[0]), list(keypoints_symmetry[1])\n",
    "    joints_left, joints_right = list([4, 5, 6, 11, 12, 13]), list([1, 2, 3, 14, 15, 16])\n",
    "\n",
    "    # normlization keypoints  Suppose using the camera parameter\n",
    "    keypoints = normalize_screen_coordinates(keypoints[..., :2], w=1000, h=1002)\n",
    "\n",
    "    model_pos = TemporalModel(17, 2, 17, filter_widths=[3, 3, 3, 3, 3], causal=args.causal, dropout=args.dropout, channels=args.channels,\n",
    "                              dense=args.dense)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model_pos = model_pos.cuda()\n",
    "\n",
    "    ckpt, time1 = ckpt_time(time0)\n",
    "    print('-------------- load data spends {:.2f} seconds'.format(ckpt))\n",
    "\n",
    "    # load trained model\n",
    "    chk_filename = os.path.join(args.checkpoint, args.resume if args.resume else args.evaluate)\n",
    "    print('Loading checkpoint', chk_filename)\n",
    "    checkpoint = torch.load(chk_filename, map_location=lambda storage, loc: storage)  # 把loc映射到storage\n",
    "    model_pos.load_state_dict(checkpoint['model_pos'])\n",
    "\n",
    "    ckpt, time2 = ckpt_time(time1)\n",
    "    print('-------------- load 3D model spends {:.2f} seconds'.format(ckpt))\n",
    "\n",
    "    #  Receptive field: 243 frames for args.arc [3, 3, 3, 3, 3]\n",
    "    receptive_field = model_pos.receptive_field()\n",
    "    pad = (receptive_field - 1) // 2  # Padding on each side\n",
    "    causal_shift = 0\n",
    "\n",
    "    print('Rendering...')\n",
    "    input_keypoints = keypoints.copy()\n",
    "    gen = UnchunkedGenerator(None, None, [input_keypoints],\n",
    "                             pad=pad, causal_shift=causal_shift, augment=args.test_time_augmentation,\n",
    "                             kps_left=kps_left, kps_right=kps_right, joints_left=joints_left, joints_right=joints_right)\n",
    "    prediction = evaluate(gen, model_pos, return_predictions=True)\n",
    "\n",
    "    # save 3D joint points 保存三维关节点\n",
    "    np.save('outputs/test_3d_output.npy', prediction, allow_pickle=True)\n",
    "\n",
    "    # 第三步：将预测的三维点从相机坐标系转换到世界坐标系\n",
    "    # （1）第一种转换方法\n",
    "    rot = np.array([0.14070565, -0.15007018, -0.7552408, 0.62232804], dtype=np.float32)\n",
    "    prediction = camera_to_world(prediction, R=rot, t=0)\n",
    "    # We don't have the trajectory, but at least we can rebase the height将预测的三维点的Z值减去预测的三维点中Z的最小值，得到正向的Z值\n",
    "    prediction[:, :, 2] -= np.min(prediction[:, :, 2])\n",
    "\n",
    "    # （2）第二种转换方法\n",
    "    # subject = 'S1'\n",
    "    # cam_id = '55011271'\n",
    "    # cam_params = load_camera_params('./camera/cameras.h5')[subject][cam_id]\n",
    "    # R = cam_params['R']\n",
    "    # T = 0\n",
    "    # azimuth = cam_params['azimuth']\n",
    "    #\n",
    "    # prediction = camera2world(pose=prediction, R=R, T=T)\n",
    "    # prediction[:, :, 2] -= np.min(prediction[:, :, 2])  # rebase the height\n",
    "\n",
    "    # 第四步：将3D关键点输出并将预测的3D点转换为bvh骨骼\n",
    "    # 将三维预测点输出\n",
    "    write_3d_point( args.viz_output,prediction)\n",
    "\n",
    "    # 将预测的三维骨骼点转换为bvh骨骼\n",
    "    prediction_copy = np.copy(prediction)\n",
    "    write_standard_bvh(args.viz_output,prediction_copy) #转为标准bvh骨骼\n",
    "    write_smartbody_bvh(args.viz_output,prediction_copy) #转为SmartBody所需的bvh骨骼\n",
    "\n",
    "    anim_output = {'Reconstruction': prediction}\n",
    "    input_keypoints = image_coordinates(input_keypoints[..., :2], w=1000, h=1002)\n",
    "\n",
    "    ckpt, time3 = ckpt_time(time2)\n",
    "    print('-------------- generate reconstruction 3D data spends {:.2f} seconds'.format(ckpt))\n",
    "\n",
    "    if not args.viz_output:\n",
    "        args.viz_output = 'outputs/outputvideo/alpha_result.mp4'\n",
    "\n",
    "    # 第五步：生成输出视频\n",
    "    # from common.visualization import render_animation\n",
    "    # render_animation(input_keypoints, anim_output,\n",
    "    #                  Skeleton(), 25, args.viz_bitrate, np.array(70., dtype=np.float32), args.viz_output,\n",
    "    #                  limit=args.viz_limit, downsample=args.viz_downsample, size=args.viz_size,\n",
    "    #                  input_video_path=args.viz_video, viewport=(1000, 1002),\n",
    "    #                  input_video_skip=args.viz_skip)\n",
    "\n",
    "    ckpt, time4 = ckpt_time(time3)\n",
    "    print('total spend {:2f} second'.format(ckpt))\n",
    "\n",
    "\n",
    "def inference_video(video_path, detector_2d):\n",
    "    \"\"\"\n",
    "    Do image -> 2d points -> 3d points to video.\n",
    "    :param detector_2d: used 2d joints detector. Can be {alpha_pose, hr_pose}\n",
    "    :param video_path: relative to outputs\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    args = parse_args()\n",
    "\n",
    "    args.detector_2d = detector_2d\n",
    "    dir_name = os.path.dirname(video_path)\n",
    "    dir_name_split = dir_name[:dir_name.rfind('/')]\n",
    "    new_dir_name = os.path.join(dir_name_split,'outputvideo')\n",
    "\n",
    "    basename = os.path.basename(video_path)\n",
    "    video_name = basename[:basename.rfind('.')]\n",
    "\n",
    "    args.viz_video = video_path\n",
    "    #args.viz_output = f'{dir_name}/{args.detector_2d}_{video_name}.mp4'\n",
    "    args.viz_output = f'{new_dir_name}/{args.detector_2d}_{video_name}.mp4'\n",
    "\n",
    "    # args.viz_limit = 20\n",
    "    # args.input_npz = 'outputs/alpha_pose_dance/dance.npz'\n",
    "\n",
    "    args.evaluate = 'pretrained_h36m_detectron_coco.bin'\n",
    "\n",
    "    with Timer(video_path):\n",
    "        main(args)\n",
    "\n",
    "# 修改视频帧率为50帧，分辨率保持不变\n",
    "def modify_video_frame_rate(videoPath,destFps):\n",
    "    dir_name = os.path.dirname(videoPath)\n",
    "    basename = os.path.basename(videoPath)\n",
    "    video_name = basename[:basename.rfind('.')]\n",
    "    video_name = video_name + \"modify_frame_rate\"\n",
    "    resultVideoPath = f'{dir_name}/{video_name}.mp4'\n",
    "\n",
    "    videoCapture = cv2.VideoCapture(videoPath)\n",
    "\n",
    "    fps = videoCapture.get(cv2.CAP_PROP_FPS)\n",
    "    if fps != destFps:\n",
    "        frameSize = (int(videoCapture.get(cv2.CAP_PROP_FRAME_WIDTH)), int(videoCapture.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "\n",
    "        videoWriter = cv2.VideoWriter(resultVideoPath,cv2.VideoWriter_fourcc('m','p','4','v'),destFps,frameSize)\n",
    "\n",
    "        i = 0;\n",
    "        print('开始转换视频帧率')\n",
    "        while True:\n",
    "            success,frame = videoCapture.read()\n",
    "            if success:\n",
    "                i+=1\n",
    "                print('转换到第%d帧'% i)\n",
    "                videoWriter.write(frame)\n",
    "            else:\n",
    "                print('视频帧转换结束')\n",
    "                break\n",
    "    return resultVideoPath\n",
    "\n",
    "# 将预测3d关键点输出到outputs/outputvideo/alpha_pose_视频名/3dpoint下\n",
    "def write_3d_point(outvideopath,prediction3dpoint):\n",
    "    '''\n",
    "    :param prediction3dpoint: 预测的三维字典\n",
    "    :param outfilepath: 输出的三维点的文件\n",
    "    :return:\n",
    "    '''\n",
    "    dir_name = os.path.dirname(outvideopath)\n",
    "    basename = os.path.basename(outvideopath)\n",
    "    video_name = basename[:basename.rfind('.')]\n",
    "\n",
    "    frameNum = 1\n",
    "\n",
    "    for frame in prediction3dpoint:\n",
    "        outfileDirectory = os.path.join(dir_name,video_name,\"3dpoint\");\n",
    "        if not os.path.exists(outfileDirectory):\n",
    "            os.makedirs(outfileDirectory)\n",
    "        outfilename = os.path.join(dir_name,video_name,\"3dpoint\",\"3dpoint{}.txt\".format(frameNum))\n",
    "        file = open(outfilename, 'w')\n",
    "        frameNum += 1\n",
    "        for point3d in frame:\n",
    "            # （1）转换成SmartBody和Meshlab的坐标系，Y轴向上，X向右，Z轴向前\n",
    "            # X = point3d[0]\n",
    "            # Y = point3d[1]\n",
    "            # Z = point3d[2]\n",
    "            #\n",
    "            # X_1 = -X\n",
    "            # Y_1 = Z\n",
    "            # Z_1 = Y\n",
    "            # str = '{},{},{}\\n'.format(X_1, Y_1, Z_1)\n",
    "\n",
    "            #（2）未转换任何坐标系的输出，Z轴向上，X向右，Y向前\n",
    "            str = '{},{},{}\\n'.format(point3d[0],point3d[1],point3d[2])\n",
    "            file.write(str)\n",
    "        file.close()\n",
    "\n",
    "# 将3dpoint转换为标准的bvh格式并输出到outputs/outputvideo/alpha_pose_视频名/bvh下\n",
    "def write_standard_bvh(outbvhfilepath,prediction3dpoint):\n",
    "    '''\n",
    "    :param outbvhfilepath: 输出bvh动作文件路径\n",
    "    :param prediction3dpoint: 预测的三维关节点\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    # 将预测的点放大100倍\n",
    "    for frame in prediction3dpoint:\n",
    "        for point3d in frame:\n",
    "            point3d[0] *= 100\n",
    "            point3d[1] *= 100\n",
    "            point3d[2] *= 100\n",
    "\n",
    "            # 交换Y和Z的坐标\n",
    "            #X = point3d[0]\n",
    "            #Y = point3d[1]\n",
    "            #Z = point3d[2]\n",
    "\n",
    "            #point3d[0] = -X\n",
    "            #point3d[1] = Z\n",
    "            #point3d[2] = Y\n",
    "\n",
    "    dir_name = os.path.dirname(outbvhfilepath)\n",
    "    basename = os.path.basename(outbvhfilepath)\n",
    "    video_name = basename[:basename.rfind('.')]\n",
    "    bvhfileDirectory = os.path.join(dir_name,video_name,\"bvh\")\n",
    "    if not os.path.exists(bvhfileDirectory):\n",
    "        os.makedirs(bvhfileDirectory)\n",
    "    bvhfileName = os.path.join(dir_name,video_name,\"bvh\",\"{}.bvh\".format(video_name))\n",
    "    human36m_skeleton = h36m_skeleton.H36mSkeleton()\n",
    "    human36m_skeleton.poses2bvh(prediction3dpoint,output_file=bvhfileName)\n",
    "\n",
    "# 将3dpoint转换为SmartBody的bvh格式并输出到outputs/outputvideo/alpha_pose_视频名/bvh下\n",
    "def write_smartbody_bvh(outbvhfilepath,prediction3dpoint):\n",
    "    '''\n",
    "    :param outbvhfilepath: 输出bvh动作文件路径\n",
    "    :param prediction3dpoint: 预测的三维关节点\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    # 将预测的点放大100倍\n",
    "    for frame in prediction3dpoint:\n",
    "        for point3d in frame:\n",
    "            # point3d[0] *= 100\n",
    "            # point3d[1] *= 100\n",
    "            # point3d[2] *= 100\n",
    "\n",
    "            # 交换Y和Z的坐标\n",
    "            X = point3d[0]\n",
    "            Y = point3d[1]\n",
    "            Z = point3d[2]\n",
    "\n",
    "            point3d[0] = -X\n",
    "            point3d[1] = Z\n",
    "            point3d[2] = Y\n",
    "\n",
    "    dir_name = os.path.dirname(outbvhfilepath)\n",
    "    basename = os.path.basename(outbvhfilepath)\n",
    "    video_name = basename[:basename.rfind('.')]\n",
    "    bvhfileDirectory = os.path.join(dir_name,video_name,\"bvh\")\n",
    "    if not os.path.exists(bvhfileDirectory):\n",
    "        os.makedirs(bvhfileDirectory)\n",
    "    bvhfileName = os.path.join(dir_name,video_name,\"bvh\",\"{}.bvh\".format(video_name))\n",
    "\n",
    "    SmartBody_skeleton = smartbody_skeleton.SmartBodySkeleton()\n",
    "    SmartBody_skeleton.poses2bvh(prediction3dpoint,output_file=bvhfileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    inference_video('outputs/inputvideo/kunkun_cut.mp4', 'alpha_pose')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
